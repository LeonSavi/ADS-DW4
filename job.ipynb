{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "03bb1eb5",
   "metadata": {},
   "source": [
    "# Libraries\n",
    "Below we import the libraries we will use later and download bla bla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "55aba5b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/leonardo/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/leonardo/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /home/leonardo/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/leonardo/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /home/leonardo/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     /home/leonardo/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "import string\n",
    "import re\n",
    "import gensim\n",
    "import inflect\n",
    "import optuna\n",
    "import pandas as pd\n",
    "from typing import Optional\n",
    "\n",
    "from nltk import pos_tag\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.porter import PorterStemmer # another 2 kinds of Stemmer (Snowball and Lancaster)\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('averaged_perceptron_tagger_eng')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d51bf3d9",
   "metadata": {},
   "source": [
    "# Data, Regex and Functions\n",
    "Below we do the following:\n",
    "- Import the data\n",
    "- Compile RegeX patterns\n",
    "- create inflect engine to convert numbers to words\n",
    "- import the Porter Stemmer (to explain why we do not use Lancaster or Snowball) and Lemmatizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "dd8503b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id', 'sentiment', 'review'], dtype='object')"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('dataset/imdb_text2vec.csv')\n",
    "# regex\n",
    "YEAR_RE = re.compile(r\"\\b((?:18|19|20)\\d{2}|[1-9]0['â€™]?s)\\b\")         \n",
    "RATING_RE = re.compile(r'\\b\\d*(?:\\.\\d+)?\\s*/\\s*(?:5|10|100)\\b')\n",
    "NUM_RE = re.compile(r'\\b\\d+(\\.\\d+)?\\b')\n",
    "SPACES_RE = re.compile(r'\\s*([?!.,]+(?:\\s+[?!.,]+)*)\\s*') #ciao,bella -> ciao, bella\n",
    "HTML_RE = re.compile(r\"<(?:\\\"[^\\\"]*\\\"['\\\"]*|'[^']*'['\\\"]*|[^'\\\">])+>\")\n",
    "# other\n",
    "p = inflect.engine()\n",
    "stemmer = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8439fd2",
   "metadata": {},
   "source": [
    "# Functions\n",
    "Here we create the core functions for our work, these are inspired from some aricles of Geek for Geeks - references at the bottom of the file. Some of these functions will be explained later as we use them, but for further details read their description. Kind of functions:\n",
    "- convert numbers\n",
    "- remove punctuations\n",
    "- fix html and spaces\n",
    "- remove stopwords\n",
    "- lematization, stemming, POS tagging\n",
    "- processing df: we gather our selected cleaning procedure in a unique function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66bf6a28",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def _convert_nums(text:str,\n",
    "        mx_splt_num:int = 1) -> str:\n",
    "\n",
    "    '''\n",
    "    We can also convert the numbers into words.\n",
    "    This can be done by using the inflect library.\n",
    "    \n",
    "    mx_num_split, we do not want conversions, like 100-> one hundred,\n",
    "    so we set the maximum number of words\n",
    "    '''\n",
    "\n",
    "    # assign <RATING>\n",
    "    text = re.sub(RATING_RE,'<RATING>',text)\n",
    "    #assign <YEAR> for 80s,90's, 1990...\n",
    "    text = re.sub(YEAR_RE,'<YEAR>',text)\n",
    "\n",
    "    # convert numbers below a threshold to string\n",
    "    temp_str = text.split()\n",
    "    new_string = []\n",
    "\n",
    "    for w in temp_str:\n",
    "\n",
    "        n2w = p.number_to_words(w) # if not a number(e.g. dog) it returns 'zero'\n",
    "        cond = (w.isdigit()) and (len(n2w.split())<=mx_splt_num)\n",
    "        \n",
    "        to_append = n2w if cond else w\n",
    "        new_string.append(to_append)\n",
    "\n",
    "    # Other Numbers become <NUM>\n",
    "    text = re.sub(NUM_RE,'<NUM>',' '.join(new_string))\n",
    "\n",
    "    return text\n",
    "\n",
    "\n",
    "def _remove_simple(text:str) -> str:\n",
    "\n",
    "    \"\"\"Does simple cleaning of HTML, and no spaces after commas\"\"\"\n",
    "\n",
    "    # remove html patterns\n",
    "  \n",
    "    text = re.sub(HTML_RE, '', text)\n",
    "\n",
    "    # remove issues like: ciao,bella\n",
    "    text = SPACES_RE.sub(lambda x: \"{} \".format(x.group(1).replace(\" \", \"\")), text)\n",
    "\n",
    "    return text\n",
    "\n",
    "\n",
    "def _remove_punct(text:str,\n",
    "                  to_keep:Optional[str] = \"\") -> str:\n",
    "    '''\n",
    "    We remove punctuations so that we don't have different forms of the same word.\n",
    "\n",
    "    '''\n",
    "    punctuation = ''.join(ch for ch in string.punctuation if ch not in to_keep)\n",
    "    translator = str.maketrans('', '', punctuation)\n",
    "\n",
    "    return text.translate(translator)\n",
    "\n",
    "\n",
    "def _remove_stopwords(text:str,\n",
    "                      other_stopwords:list[str]|str = None,\n",
    "                      to_keep:list[str]|str = None,\n",
    "                      lst_output:bool=False) -> list[str]:\n",
    "\n",
    "    '''\n",
    "    Stopwords are words that do not contribute much to the meaning of a sentence\n",
    "    hence they can be removed. The NLTK library has a set of \n",
    "    stopwords and we can use these to remove stopwords from our text.     \n",
    "    '''\n",
    "    \n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "    if other_stopwords: stop_words.update(set(other_stopwords))\n",
    "    if to_keep: stop_words.discard(set(to_keep))\n",
    "\n",
    "    word_tokens = word_tokenize(text)\n",
    "    clean = [w for w in word_tokens if not w.lower() in stop_words]\n",
    "    return clean if lst_output else ' '.join(clean)\n",
    "\n",
    "    \n",
    "def _stem_words(text:list[str]|str,\n",
    "                lst_output:bool=True) -> list[str]:\n",
    "    '''\n",
    "    Stemming is the process of getting the root form of a word.\n",
    "    Stem or root is the part to which affixes like -ed, -ize, -de, -s, etc are added.\n",
    "    The stem of a word is created by removing the prefix or suffix of a word.\n",
    "    \n",
    "    '''\n",
    "    word_tokens = text if isinstance(text,list) else word_tokenize(text)\n",
    "    stems = [stemmer.stem(word) for word in word_tokens]\n",
    "    return stems if lst_output else ' '.join(stems)\n",
    "\n",
    "\n",
    "def _lemma_words(text:str,\n",
    "                 lst_output:bool=True) -> list[str]:\n",
    "    '''\n",
    "    Lemmatization is an NLP technique that reduces a word to its root form.\n",
    "    This can be helpful for tasks such as text analysis and search as it\n",
    "    allows us to compare words that are related but have different forms.\n",
    "\n",
    "    combines pos_tagging with wordnet_pos for better lemma\n",
    "    \n",
    "    '''\n",
    "\n",
    "    iterable = _pos_tagging(text)\n",
    "\n",
    "    lemmas = [lemmatizer.lemmatize(w,_get_wordnet_pos(p)) for w, p in iterable]\n",
    "\n",
    "    return lemmas if lst_output else ' '.join(lemmas)\n",
    "\n",
    "\n",
    "def _pos_tagging(text):\n",
    "    '''\n",
    "    POS tagging is the process of assigning each word in a sentence its grammatical category,\n",
    "    such as noun, verb, adjective or adverb. It helps machines understand the structure \n",
    "    and meaning of text, enabling tasks like parsing, information extraction and text analysis.\n",
    "\n",
    "    NNP: Proper noun\n",
    "    NN: Noun (singular)\n",
    "    VBZ: Verb (3rd person singular)\n",
    "    CC: Conjunction\n",
    "\n",
    "\n",
    "    '''\n",
    "    word_tokens = word_tokenize(text)\n",
    "    return pos_tag(word_tokens)\n",
    "\n",
    "\n",
    "def _get_wordnet_pos(treebank_tag):\n",
    "\n",
    "    '''\n",
    "    Get wordnet positioning: https://stackoverflow.com/questions/15586721/wordnet-lemmatization-and-pos-tagging-in-python\n",
    "    '''\n",
    "\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN  #default\n",
    "\n",
    "\n",
    "def preprocess_df(df:pd.DataFrame\n",
    "                  ) -> pd.DataFrame:\n",
    "    '''\n",
    "    Perform cleaning on reviews:\n",
    "    https://www.geeksforgeeks.org/machine-learning/text-preprocessing-in-python-set-1/\n",
    "\n",
    "    '''\n",
    "    # 0.  x.lower().split() -> lower case and remove extra spaces,\n",
    "    # also number conversion (see above)\n",
    "    df['num_clean'] = df['review'].apply(lambda x: _convert_nums(\" \".join(x.lower().split())))\n",
    "\n",
    "    # 1 remove html and ciao,bella->ciao, bella\n",
    "    df['first_l_cl'] = df['num_clean'].apply(lambda x: _remove_simple(x))\n",
    "\n",
    "    # 2 remove punctuation\n",
    "    df['sec_l_cl'] = df['first_l_cl'].apply(lambda x: _remove_punct(x))\n",
    "\n",
    "    # 3 remove stopwords \n",
    "    df['no_stopw'] = df['sec_l_cl'].apply(lambda x: _remove_stopwords(x,lst_output= False))\n",
    "\n",
    "    # 4a lemmas \n",
    "    df['lemmas'] = df['no_stopw'].apply(lambda x: _lemma_words(x, lst_output=True))\n",
    "\n",
    "    # 5a stemming\n",
    "    df['stem'] = df['no_stopw'].apply(lambda x: _stem_words(x, lst_output=True))\n",
    "\n",
    "    return df\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3224a1e",
   "metadata": {},
   "source": [
    "# Dataset clearning steps\n",
    "Our function preprocess_df, picks up the main cleaning steps in the following order\n",
    "1. We set to lower case and remove unnecessary blanks\n",
    "2. we convert numbers to strings if below 100, ratings (e.g. 7.5/10,60/100) become < RATING > while expression of years (e.g. 80s/2000) become < YEAR >, while other numbers becoome < NUM > so that we do not loose information\n",
    "3. Upon taking care of numbers, we do a simple clearning to fix some issues: such as html words (e.g. <\\b>) and to add forgotten blanks after commas.\n",
    "4. We can now remove the whole punctuations\n",
    "5. We remove stopwords\n",
    "6. we can apply lemmatisation or stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0defc7f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# it takes a bit to process, but i m too lazy to optimize \n",
    "\n",
    "cleaned = preprocess_df(df)\n",
    "\n",
    "cleaned.head(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f40008e4",
   "metadata": {},
   "source": [
    "# Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a70f6af2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TR: word2vec TF-IDF: https://www.geeksforgeeks.org/python/python-word-embedding-using-word2vec/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9057a70b",
   "metadata": {},
   "source": [
    "# Models Setup \n",
    "LS: doing some random stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12bb567b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dw4_py",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
